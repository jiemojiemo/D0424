{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student1/anaconda3/envs/keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Test data was loaded\n",
      "\n",
      "Loading training data...\n",
      "Training data was loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# python train_model.py model={iphone,sony,blackberry} dped_dir=dped vgg_dir=vgg_pretrained/imagenet-vgg-verydeep-19.mat\n",
    "\n",
    "import tensorflow as tf\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from load_dataset import load_test_data, load_batch\n",
    "from ssim import MultiScaleSSIM\n",
    "import models\n",
    "import utils\n",
    "import vgg\n",
    "\n",
    "from tensorflow.python.keras.applications import VGG16\n",
    "from tensorflow.python.keras.layers import Dropout, Dense\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "#set_session(tf.Session(config=config))\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "def mean_score(dis):\n",
    "    score_range = tf.range(0, 10, 1, dtype=tf.float32)\n",
    "    score = tf.matmul(dis, tf.reshape(score_range, [-1, 1]))\n",
    "    return score\n",
    "def nima_preprocess_input(img):\n",
    "    img = img / 127.5\n",
    "    img = img - 1.0\n",
    "    return img\n",
    "\n",
    "# defining size of the training image patches\n",
    "\n",
    "PATCH_WIDTH = 100\n",
    "PATCH_HEIGHT = 100\n",
    "PATCH_SIZE = PATCH_WIDTH * PATCH_HEIGHT * 3\n",
    "\n",
    "# processing command arguments\n",
    "\n",
    "batch_size = 20\n",
    "train_size = 30000\n",
    "learning_rate = 5e-4\n",
    "num_train_iters = 20000\n",
    "\n",
    "w_content = 100.0\n",
    "w_color = 0.5\n",
    "w_texture = 1\n",
    "w_tv = 2000\n",
    "w_nima = 10\n",
    "\n",
    "dped_dir = '/home/public/hw/dataset/dped/dped/'\n",
    "vgg_dir = 'vgg_pretrained/imagenet-vgg-verydeep-19.mat'\n",
    "eval_step = 1000\n",
    "\n",
    "phone = \"my\"\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# loading training and test data\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_data, test_answ = load_test_data(phone, dped_dir, PATCH_SIZE)\n",
    "print(\"Test data was loaded\\n\")\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "train_data, train_answ = load_batch(phone, dped_dir, train_size, PATCH_SIZE)\n",
    "print(\"Training data was loaded\\n\")\n",
    "\n",
    "TEST_SIZE = test_data.shape[0]\n",
    "num_test_batches = int(test_data.shape[0]/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing variables\n",
      "Loading weights of nima\n",
      "Training network\n",
      "step 0, my | discriminator accuracy | train: 0.00035, test: 0.4923\n",
      "generator losses | train: 0.5818, test: 553.2 | content: 1.509, color: 796.3, texture: -52.91, tv: 0.0001055 |                         psnr: 11.25, nima: 5.689 ssim: 0.3962\n",
      " nima_mean: 3.311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student1/anaconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py:200: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000, my | discriminator accuracy | train: 0.5013, test: 0.5052\n",
      "generator losses | train: 109.3, test: 74.28 | content: 0.2128, color: 29.69, texture: -14.55, tv: 0.004962 |                         psnr: 22.98, nima: 4.278 ssim: 0.937\n",
      " nima_mean: 4.722\n",
      "step 2000, my | discriminator accuracy | train: 0.5004, test: 0.5003\n",
      "generator losses | train: 70.63, test: 65.54 | content: 0.1796, color: 22.64, texture: -14.31, tv: 0.004178 |                         psnr: 23.47, nima: 4.221 ssim: 0.9396\n",
      " nima_mean: 4.779\n",
      "step 3000, my | discriminator accuracy | train: 0.4977, test: 0.4984\n",
      "generator losses | train: 64.79, test: 69.76 | content: 0.1715, color: 31.37, texture: -14.02, tv: 0.004493 |                         psnr: 23.03, nima: 4.197 ssim: 0.9412\n",
      " nima_mean: 4.803\n",
      "step 4000, my | discriminator accuracy | train: 0.4964, test: 0.4988\n",
      "generator losses | train: 63.19, test: 60.15 | content: 0.1472, color: 18.32, texture: -14.79, tv: 0.005263 |                         psnr: 24.1, nima: 4.054 ssim: 0.9382\n",
      " nima_mean: 4.946\n",
      "step 5000, my | discriminator accuracy | train: 0.5029, test: 0.4931\n",
      "generator losses | train: 61.15, test: 58.57 | content: 0.147, color: 16.77, texture: -14.34, tv: 0.00458 |                         psnr: 24.17, nima: 4.067 ssim: 0.9385\n",
      " nima_mean: 4.933\n",
      "step 6000, my | discriminator accuracy | train: 0.507, test: 0.5021\n",
      "generator losses | train: 60.33, test: 58.91 | content: 0.1451, color: 17.32, texture: -13.95, tv: 0.004821 |                         psnr: 24.2, nima: 4.005 ssim: 0.9383\n",
      " nima_mean: 4.995\n",
      "step 7000, my | discriminator accuracy | train: 0.5101, test: 0.5291\n",
      "generator losses | train: 59.28, test: 61.12 | content: 0.1461, color: 22.38, texture: -13.81, tv: 0.004531 |                         psnr: 23.55, nima: 4.007 ssim: 0.9324\n",
      " nima_mean: 4.993\n",
      "step 8000, my | discriminator accuracy | train: 0.5092, test: 0.5308\n",
      "generator losses | train: 58.24, test: 56.87 | content: 0.1376, color: 15.73, texture: -13.81, tv: 0.004692 |                         psnr: 24.25, nima: 3.967 ssim: 0.9331\n",
      " nima_mean: 5.033\n",
      "step 9000, my | discriminator accuracy | train: 0.5188, test: 0.528\n",
      "generator losses | train: 57.57, test: 56.9 | content: 0.1292, color: 16.09, texture: -13.84, tv: 0.005122 |                         psnr: 24.34, nima: 3.953 ssim: 0.9366\n",
      " nima_mean: 5.047\n",
      "step 10000, my | discriminator accuracy | train: 0.5359, test: 0.5081\n",
      "generator losses | train: 57.52, test: 57.55 | content: 0.1292, color: 18.15, texture: -13.9, tv: 0.004948 |                         psnr: 24.15, nima: 3.956 ssim: 0.9401\n",
      " nima_mean: 5.044\n",
      "step 11000, my | discriminator accuracy | train: 0.5367, test: 0.5581\n",
      "generator losses | train: 57.07, test: 56.35 | content: 0.1324, color: 15.65, texture: -13.68, tv: 0.004817 |                         psnr: 24.39, nima: 3.934 ssim: 0.94\n",
      " nima_mean: 5.066\n",
      "step 12000, my | discriminator accuracy | train: 0.5422, test: 0.5389\n",
      "generator losses | train: 56.66, test: 55.73 | content: 0.1252, color: 14.6, texture: -13.7, tv: 0.005249 |                         psnr: 24.53, nima: 3.911 ssim: 0.9424\n",
      " nima_mean: 5.089\n",
      "step 13000, my | discriminator accuracy | train: 0.5521, test: 0.5757\n",
      "generator losses | train: 56.02, test: 55.19 | content: 0.1304, color: 13.54, texture: -13.48, tv: 0.00495 |                         psnr: 24.45, nima: 3.896 ssim: 0.9305\n",
      " nima_mean: 5.104\n",
      "step 14000, my | discriminator accuracy | train: 0.5578, test: 0.5671\n",
      "generator losses | train: 56.43, test: 58.78 | content: 0.1279, color: 19.02, texture: -13.54, tv: 0.005319 |                         psnr: 24.11, nima: 3.938 ssim: 0.9414\n",
      " nima_mean: 5.062\n",
      "step 15000, my | discriminator accuracy | train: 0.5667, test: 0.5962\n",
      "generator losses | train: 56.13, test: 55.18 | content: 0.1256, color: 13.62, texture: -13.32, tv: 0.005131 |                         psnr: 24.48, nima: 3.887 ssim: 0.9365\n",
      " nima_mean: 5.113\n",
      "step 16000, my | discriminator accuracy | train: 0.563, test: 0.5658\n",
      "generator losses | train: 55.58, test: 55.08 | content: 0.1269, color: 13.75, texture: -13.43, tv: 0.005235 |                         psnr: 24.46, nima: 3.848 ssim: 0.934\n",
      " nima_mean: 5.152\n",
      "step 17000, my | discriminator accuracy | train: 0.568, test: 0.5836\n",
      "generator losses | train: 55.88, test: 56.35 | content: 0.1239, color: 14.96, texture: -13.24, tv: 0.005403 |                         psnr: 24.46, nima: 3.891 ssim: 0.9355\n",
      " nima_mean: 5.109\n",
      "step 18000, my | discriminator accuracy | train: 0.5752, test: 0.5881\n",
      "generator losses | train: 55.49, test: 56.44 | content: 0.1231, color: 15.92, texture: -13.16, tv: 0.005411 |                         psnr: 24.36, nima: 3.851 ssim: 0.9365\n",
      " nima_mean: 5.149\n",
      "step 19000, my | discriminator accuracy | train: 0.5749, test: 0.6029\n",
      "generator losses | train: 55.62, test: 54.89 | content: 0.1181, color: 13.66, texture: -13.2, tv: 0.005343 |                         psnr: 24.66, nima: 3.876 ssim: 0.9385\n",
      " nima_mean: 5.124\n",
      "100% done\r"
     ]
    }
   ],
   "source": [
    "# defining system architecture\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session(config=config) as sess:\n",
    "    \n",
    "    # placeholders for training data\n",
    "\n",
    "    phone_ = tf.placeholder(tf.float32, [None, PATCH_SIZE])\n",
    "    phone_image = tf.reshape(phone_, [-1, PATCH_HEIGHT, PATCH_WIDTH, 3])\n",
    "\n",
    "    dslr_ = tf.placeholder(tf.float32, [None, PATCH_SIZE])\n",
    "    dslr_image = tf.reshape(dslr_, [-1, PATCH_HEIGHT, PATCH_WIDTH, 3])\n",
    "\n",
    "    adv_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    # get processed enhanced image\n",
    "\n",
    "    enhanced = models.resnet(phone_image)\n",
    "\n",
    "    # transform both dslr and enhanced images to grayscale\n",
    "\n",
    "    enhanced_gray = tf.reshape(tf.image.rgb_to_grayscale(enhanced), [-1, PATCH_WIDTH * PATCH_HEIGHT])\n",
    "    dslr_gray = tf.reshape(tf.image.rgb_to_grayscale(dslr_image),[-1, PATCH_WIDTH * PATCH_HEIGHT])\n",
    "\n",
    "    # push randomly the enhanced or dslr image to an adversarial CNN-discriminator\n",
    "\n",
    "    adversarial_ = tf.multiply(enhanced_gray, 1 - adv_) + tf.multiply(dslr_gray, adv_)\n",
    "    adversarial_image = tf.reshape(adversarial_, [-1, PATCH_HEIGHT, PATCH_WIDTH, 1])\n",
    "\n",
    "    discrim_predictions = models.adversarial(adversarial_image)\n",
    "\n",
    "    # losses\n",
    "    # 1) texture (adversarial) loss\n",
    "\n",
    "    discrim_target = tf.concat([adv_, 1 - adv_], 1)\n",
    "\n",
    "    loss_discrim = -tf.reduce_sum(discrim_target * tf.log(tf.clip_by_value(discrim_predictions, 1e-10, 1.0)))\n",
    "    loss_texture = -loss_discrim\n",
    "\n",
    "    correct_predictions = tf.equal(tf.argmax(discrim_predictions, 1), tf.argmax(discrim_target, 1))\n",
    "    discim_accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "\n",
    "    # 2) content loss\n",
    "\n",
    "    CONTENT_LAYER = 'pool3'\n",
    "\n",
    "    enhanced_vgg = vgg.net(vgg_dir, vgg.preprocess(enhanced * 255))\n",
    "    dslr_vgg = vgg.net(vgg_dir, vgg.preprocess(dslr_image * 255))\n",
    "\n",
    "    content_size = utils._tensor_size(dslr_vgg[CONTENT_LAYER]) * batch_size\n",
    "    loss_content = 2 * tf.nn.l2_loss(enhanced_vgg[CONTENT_LAYER]/content_size - dslr_vgg[CONTENT_LAYER]/content_size) \n",
    "\n",
    "    # 3) color loss\n",
    "\n",
    "    enhanced_blur = utils.blur(enhanced)\n",
    "    dslr_blur = utils.blur(dslr_image)\n",
    "\n",
    "    loss_color = tf.reduce_sum(tf.pow(dslr_blur - enhanced_blur, 2))/(2 * batch_size)\n",
    "\n",
    "    # 4) total variation loss\n",
    "\n",
    "    batch_shape = (batch_size, PATCH_WIDTH, PATCH_HEIGHT, 3)\n",
    "    tv_y_size = utils._tensor_size(enhanced[:,1:,:,:])\n",
    "    tv_x_size = utils._tensor_size(enhanced[:,:,1:,:])\n",
    "    y_tv = tf.nn.l2_loss(enhanced[:,1:,:,:] - enhanced[:,:batch_shape[1]-1,:,:])\n",
    "    x_tv = tf.nn.l2_loss(enhanced[:,:,1:,:] - enhanced[:,:,:batch_shape[2]-1,:])\n",
    "    loss_tv = 2 * (x_tv/tv_x_size + y_tv/tv_y_size) / batch_size\n",
    "    \n",
    "    # 5) nima loss\n",
    "    base_model = VGG16(input_shape=(None, None, 3), include_top=False, pooling='avg', weights=None)\n",
    "    x = Dropout(0.75)(base_model.output)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    nima_model = Model(base_model.input, x)\n",
    "    nima_model.trainable = False\n",
    "    for layer in nima_model.layers:\n",
    "        layer.trainable = False\n",
    "    nima_outputs = nima_model(nima_preprocess_input(enhanced * 255.))\n",
    "    nima_score = tf.reduce_mean(mean_score(nima_outputs))\n",
    "    loss_nima = 9.0 - nima_score\n",
    "\n",
    "    # final loss\n",
    "\n",
    "    loss_generator = w_content * loss_content + w_texture * loss_texture + w_color * loss_color + w_tv * loss_tv + w_nima*loss_nima\n",
    "\n",
    "    # psnr loss\n",
    "\n",
    "    enhanced_flat = tf.reshape(enhanced, [-1, PATCH_SIZE])\n",
    "\n",
    "    loss_mse = tf.reduce_sum(tf.pow(dslr_ - enhanced_flat, 2))/(PATCH_SIZE * batch_size)\n",
    "    loss_psnr = 20 * utils.log10(1.0 / tf.sqrt(loss_mse))\n",
    "\n",
    "    # optimize parameters of image enhancement (generator) and discriminator networks\n",
    "\n",
    "    generator_vars = [v for v in tf.global_variables() if v.name.startswith(\"generator\")]\n",
    "    discriminator_vars = [v for v in tf.global_variables() if v.name.startswith(\"discriminator\")]\n",
    "\n",
    "    train_step_gen = tf.train.AdamOptimizer(learning_rate).minimize(loss_generator, var_list=generator_vars)\n",
    "    train_step_disc = tf.train.AdamOptimizer(learning_rate).minimize(loss_discrim, var_list=discriminator_vars)\n",
    "\n",
    "    saver = tf.train.Saver(var_list=generator_vars, max_to_keep=100)\n",
    "\n",
    "    print('Initializing variables')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Loading weights of nima')\n",
    "    nima_model.load_weights('../myproject/NIMA_weights/vgg16_weights.h5')\n",
    "\n",
    "    print('Training network')\n",
    "\n",
    "    train_loss_gen = 0.0\n",
    "    train_acc_discrim = 0.0\n",
    "\n",
    "    all_zeros = np.reshape(np.zeros((batch_size, 1)), [batch_size, 1])\n",
    "    test_index = np.random.randint(0, TEST_SIZE, 5);\n",
    "    test_crops = test_data[test_index, :]\n",
    "    test_dslr_crops = test_answ[test_index, :]\n",
    "\n",
    "    logs = open('mymodels/' + phone + '.txt', \"w+\")\n",
    "    logs.write('w_content:{}, w_texture:{}, w_color:{}, w_tv:{}, w_nima:{}'.format(w_content, w_texture, w_color, w_tv, w_nima))\n",
    "    logs.close()\n",
    "\n",
    "    for i in range(num_train_iters):\n",
    "\n",
    "        # train generator\n",
    "\n",
    "        idx_train = np.random.randint(0, train_size, batch_size)\n",
    "\n",
    "        phone_images = train_data[idx_train]\n",
    "        dslr_images = train_answ[idx_train]\n",
    "\n",
    "        [loss_temp, temp] = sess.run([loss_generator, train_step_gen],\n",
    "                                        feed_dict={phone_: phone_images, dslr_: dslr_images, adv_: all_zeros, K.learning_phase():0})\n",
    "        train_loss_gen += loss_temp / eval_step\n",
    "\n",
    "        # train discriminator\n",
    "\n",
    "        idx_train = np.random.randint(0, train_size, batch_size)\n",
    "\n",
    "        # generate image swaps (dslr or enhanced) for discriminator\n",
    "        swaps = np.reshape(np.random.randint(0, 2, batch_size), [batch_size, 1])\n",
    "\n",
    "        phone_images = train_data[idx_train]\n",
    "        dslr_images = train_answ[idx_train]\n",
    "\n",
    "        [accuracy_temp, temp] = sess.run([discim_accuracy, train_step_disc],\n",
    "                                        feed_dict={phone_: phone_images, dslr_: dslr_images, adv_: swaps, K.learning_phase():0})\n",
    "        train_acc_discrim += accuracy_temp / eval_step\n",
    "\n",
    "        if i % eval_step == 0:\n",
    "\n",
    "            # test generator and discriminator CNNs\n",
    "\n",
    "            test_losses_gen = np.zeros((1, 7))\n",
    "            test_accuracy_disc = 0.0\n",
    "            loss_ssim = 0.0\n",
    "            nima_mean_scores = 0.0\n",
    "\n",
    "            for j in range(num_test_batches):\n",
    "\n",
    "                be = j * batch_size\n",
    "                en = (j+1) * batch_size\n",
    "\n",
    "                swaps = np.reshape(np.random.randint(0, 2, batch_size), [batch_size, 1])\n",
    "\n",
    "                phone_images = test_data[be:en]\n",
    "                dslr_images = test_answ[be:en]\n",
    "\n",
    "                [enhanced_crops, accuracy_disc, losses, nima_mean] = sess.run([enhanced, discim_accuracy, \\\n",
    "                                [loss_generator, loss_content, loss_color, loss_texture, loss_tv, loss_psnr, loss_nima], nima_score], \\\n",
    "                                feed_dict={phone_: phone_images, dslr_: dslr_images, adv_: swaps, K.learning_phase():0})\n",
    "\n",
    "                test_losses_gen += np.asarray(losses) / num_test_batches\n",
    "                test_accuracy_disc += accuracy_disc / num_test_batches\n",
    "                nima_mean_scores += nima_mean / num_test_batches\n",
    "                \n",
    "                loss_ssim += MultiScaleSSIM(np.reshape(dslr_images * 255, [batch_size, PATCH_HEIGHT, PATCH_WIDTH, 3]),\n",
    "                                                    enhanced_crops * 255) / num_test_batches\n",
    "\n",
    "            logs_disc = \"step %d, %s | discriminator accuracy | train: %.4g, test: %.4g\" % \\\n",
    "                  (i, phone, train_acc_discrim, test_accuracy_disc)\n",
    "\n",
    "            logs_gen = \"generator losses | train: %.4g, test: %.4g | content: %.4g, color: %.4g, texture: %.4g, tv: %.4g | \\\n",
    "                        psnr: %.4g, nima: %.4g ssim: %.4g\\n nima_mean: %.4g\" % \\\n",
    "                  (train_loss_gen, test_losses_gen[0][0], test_losses_gen[0][1], test_losses_gen[0][2],\n",
    "                   test_losses_gen[0][3], test_losses_gen[0][4], test_losses_gen[0][5], test_losses_gen[0][6],loss_ssim, nima_mean_scores)\n",
    "\n",
    "            print(logs_disc)\n",
    "            print(logs_gen)\n",
    "\n",
    "            # save the results to log file\n",
    "\n",
    "            logs = open('mymodels/' + phone + '.txt', \"a\")\n",
    "            logs.write(logs_disc)\n",
    "            logs.write('\\n')\n",
    "            logs.write(logs_gen)\n",
    "            logs.write('\\n')\n",
    "            logs.close()\n",
    "\n",
    "            # save visual results for several test image crops\n",
    "\n",
    "            enhanced_crops = sess.run(enhanced, feed_dict={phone_: test_crops, dslr_: dslr_images, adv_: all_zeros, K.learning_phase():0})\n",
    "\n",
    "            idx = 0\n",
    "            for crop in enhanced_crops:\n",
    "                before_after = np.hstack((np.reshape(test_crops[idx], [PATCH_HEIGHT, PATCH_WIDTH, 3]), crop, \n",
    "                                          np.reshape(test_dslr_crops[idx], [PATCH_HEIGHT, PATCH_WIDTH, 3])))\n",
    "                misc.imsave('myresults/' + str(phone)+ \"_\" + str(idx) + '_iteration_' + str(i) + '.jpg', before_after)\n",
    "                idx += 1\n",
    "\n",
    "            train_loss_gen = 0.0\n",
    "            train_acc_discrim = 0.0\n",
    "\n",
    "            # save the model that corresponds to the current iteration\n",
    "\n",
    "            saver.save(sess, 'mymodels/' + str(phone) + '_iteration_' + str(i) + '.ckpt', write_meta_graph=False)\n",
    "\n",
    "            # reload a different batch of training data\n",
    "\n",
    "            del train_data\n",
    "            del train_answ\n",
    "            train_data, train_answ = load_batch(phone, dped_dir, train_size, PATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
