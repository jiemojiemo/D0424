{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student1/anaconda3/envs/keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Test data was loaded\n",
      "\n",
      "Loading training data...\n",
      "Training data was loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# python train_model.py model={iphone,sony,blackberry} dped_dir=dped vgg_dir=vgg_pretrained/imagenet-vgg-verydeep-19.mat\n",
    "\n",
    "import tensorflow as tf\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from load_dataset import load_test_data, load_batch\n",
    "from ssim import MultiScaleSSIM\n",
    "import models\n",
    "import utils\n",
    "import vgg\n",
    "\n",
    "from tensorflow.python.keras.applications import VGG16\n",
    "from tensorflow.python.keras.layers import Dropout, Dense\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "#set_session(tf.Session(config=config))\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "def mean_score(dis):\n",
    "    score_range = tf.range(0, 10, 1, dtype=tf.float32)\n",
    "    score = tf.matmul(dis, tf.reshape(score_range, [-1, 1]))\n",
    "    return score\n",
    "def nima_preprocess_input(img):\n",
    "    img = img / 127.5\n",
    "    img = img - 1.0\n",
    "    return img\n",
    "\n",
    "# defining size of the training image patches\n",
    "\n",
    "PATCH_WIDTH = 100\n",
    "PATCH_HEIGHT = 100\n",
    "PATCH_SIZE = PATCH_WIDTH * PATCH_HEIGHT * 3\n",
    "\n",
    "# processing command arguments\n",
    "\n",
    "batch_size = 20\n",
    "train_size = 30000\n",
    "learning_rate = 5e-4\n",
    "num_train_iters = 20000\n",
    "\n",
    "w_content = 10\n",
    "w_color = 0.5\n",
    "w_texture = 1\n",
    "w_tv = 2000\n",
    "\n",
    "# without nima\n",
    "w_nima = 0\n",
    "\n",
    "dped_dir = '/home/public/hw/dataset/dped/dped/'\n",
    "vgg_dir = 'vgg_pretrained/imagenet-vgg-verydeep-19.mat'\n",
    "eval_step = 1000\n",
    "\n",
    "phone = \"my\"\n",
    "postfix = 'withoutNIMA'\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# loading training and test data\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_data, test_answ = load_test_data(phone, dped_dir, PATCH_SIZE)\n",
    "print(\"Test data was loaded\\n\")\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "train_data, train_answ = load_batch(phone, dped_dir, train_size, PATCH_SIZE)\n",
    "print(\"Training data was loaded\\n\")\n",
    "\n",
    "TEST_SIZE = test_data.shape[0]\n",
    "num_test_batches = int(test_data.shape[0]/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing variables\n",
      "Loading weights of nima\n",
      "Training network\n",
      "step 0, my | discriminator accuracy | train: 0.00075, test: 0.4923\n",
      "generator losses | train: 0.6988, test: 659.3 | content: 28.87, color: 820.3, texture: -39.65, tv: 5.737e-05 |                         psnr: 11.11, nima: 115.6 ssim: 0.319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student1/anaconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py:195: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000, my | discriminator accuracy | train: 0.5017, test: 0.5027\n",
      "generator losses | train: 202.9, test: 133.7 | content: 12.09, color: 35.7, texture: -14.48, tv: 0.004704 |                         psnr: 22.64, nima: 95.37 ssim: 0.9367\n",
      "\n",
      "step 2000, my | discriminator accuracy | train: 0.4998, test: 0.4945\n",
      "generator losses | train: 108.6, test: 99.73 | content: 8.928, color: 26.93, texture: -14.24, tv: 0.005613 |                         psnr: 23.84, nima: 93.97 ssim: 0.9502\n",
      "\n",
      "step 3000, my | discriminator accuracy | train: 0.5047, test: 0.5128\n",
      "generator losses | train: 90.22, test: 94.23 | content: 8.352, color: 26.92, texture: -13.99, tv: 0.005622 |                         psnr: 23.98, nima: 94.4 ssim: 0.954\n",
      "\n",
      "step 4000, my | discriminator accuracy | train: 0.5024, test: 0.5012\n",
      "generator losses | train: 84.96, test: 83.16 | content: 7.273, color: 20.89, texture: -14.9, tv: 0.007445 |                         psnr: 24.85, nima: 92.63 ssim: 0.959\n",
      "\n",
      "step 5000, my | discriminator accuracy | train: 0.5057, test: 0.4938\n",
      "generator losses | train: 78.81, test: 79.77 | content: 7.148, color: 21.83, texture: -14.21, tv: 0.005794 |                         psnr: 24.65, nima: 93.72 ssim: 0.9589\n",
      "\n",
      "step 6000, my | discriminator accuracy | train: 0.5086, test: 0.5033\n",
      "generator losses | train: 73.73, test: 69.78 | content: 6.235, color: 18.21, texture: -13.88, tv: 0.0061 |                         psnr: 25.2, nima: 93.68 ssim: 0.9612\n",
      "\n",
      "step 7000, my | discriminator accuracy | train: 0.5165, test: 0.5427\n",
      "generator losses | train: 69.56, test: 74.13 | content: 6.184, color: 27.86, texture: -13.73, tv: 0.006047 |                         psnr: 24.24, nima: 93.63 ssim: 0.96\n",
      "\n",
      "step 8000, my | discriminator accuracy | train: 0.5105, test: 0.4988\n",
      "generator losses | train: 66.78, test: 68.69 | content: 5.994, color: 19.07, texture: -13.87, tv: 0.00654 |                         psnr: 25.3, nima: 93.81 ssim: 0.9647\n",
      "\n",
      "step 9000, my | discriminator accuracy | train: 0.5182, test: 0.5573\n",
      "generator losses | train: 64.3, test: 63.12 | content: 5.581, color: 18.35, texture: -13.73, tv: 0.005937 |                         psnr: 25.17, nima: 93.48 ssim: 0.9612\n",
      "\n",
      "step 10000, my | discriminator accuracy | train: 0.5289, test: 0.5347\n",
      "generator losses | train: 62.56, test: 66.25 | content: 5.811, color: 19.76, texture: -13.85, tv: 0.006048 |                         psnr: 25.17, nima: 93.79 ssim: 0.9641\n",
      "\n",
      "step 11000, my | discriminator accuracy | train: 0.5277, test: 0.5493\n",
      "generator losses | train: 60.92, test: 59.68 | content: 5.336, color: 15.95, texture: -13.74, tv: 0.006043 |                         psnr: 25.67, nima: 93.43 ssim: 0.9658\n",
      "\n",
      "step 12000, my | discriminator accuracy | train: 0.5266, test: 0.5257\n",
      "generator losses | train: 59.71, test: 63.61 | content: 5.474, color: 21.13, texture: -13.83, tv: 0.006064 |                         psnr: 24.98, nima: 93.22 ssim: 0.9642\n",
      "\n",
      "step 13000, my | discriminator accuracy | train: 0.5356, test: 0.5322\n",
      "generator losses | train: 57.53, test: 57.53 | content: 5.074, color: 15.24, texture: -13.79, tv: 0.00648 |                         psnr: 25.9, nima: 93.61 ssim: 0.9676\n",
      "\n",
      "step 14000, my | discriminator accuracy | train: 0.5407, test: 0.5356\n",
      "generator losses | train: 56.68, test: 68.63 | content: 5.362, color: 27.15, texture: -13.73, tv: 0.007582 |                         psnr: 24.66, nima: 92.34 ssim: 0.9657\n",
      "\n",
      "step 15000, my | discriminator accuracy | train: 0.5355, test: 0.5592\n",
      "generator losses | train: 55.56, test: 56.28 | content: 5.018, color: 15.05, texture: -13.69, tv: 0.006129 |                         psnr: 25.76, nima: 93.92 ssim: 0.9648\n",
      "\n",
      "step 16000, my | discriminator accuracy | train: 0.5285, test: 0.5209\n",
      "generator losses | train: 54.38, test: 54.74 | content: 4.785, color: 15.95, texture: -13.84, tv: 0.00638 |                         psnr: 25.77, nima: 93.15 ssim: 0.9661\n",
      "\n",
      "step 17000, my | discriminator accuracy | train: 0.5341, test: 0.5183\n",
      "generator losses | train: 54.52, test: 56.12 | content: 5.039, color: 14.63, texture: -13.77, tv: 0.006093 |                         psnr: 25.88, nima: 93.52 ssim: 0.967\n",
      "\n",
      "step 18000, my | discriminator accuracy | train: 0.5352, test: 0.5527\n",
      "generator losses | train: 53.06, test: 54.63 | content: 4.793, color: 15.52, texture: -13.68, tv: 0.006305 |                         psnr: 25.74, nima: 93.21 ssim: 0.9653\n",
      "\n",
      "step 19000, my | discriminator accuracy | train: 0.5339, test: 0.5321\n",
      "generator losses | train: 52.7, test: 53.61 | content: 4.738, color: 15.15, texture: -13.82, tv: 0.006237 |                         psnr: 25.89, nima: 93.38 ssim: 0.9678\n",
      "\n",
      "100% done\r"
     ]
    }
   ],
   "source": [
    "# defining system architecture\n",
    "models_dir = 'models/withoutNIMA/'\n",
    "results_dir = 'results/withoutNIMA/'\n",
    "with tf.Graph().as_default(), tf.Session(config=config) as sess:\n",
    "    \n",
    "    # placeholders for training data\n",
    "\n",
    "    phone_ = tf.placeholder(tf.float32, [None, PATCH_SIZE])\n",
    "    phone_image = tf.reshape(phone_, [-1, PATCH_HEIGHT, PATCH_WIDTH, 3])\n",
    "\n",
    "    dslr_ = tf.placeholder(tf.float32, [None, PATCH_SIZE])\n",
    "    dslr_image = tf.reshape(dslr_, [-1, PATCH_HEIGHT, PATCH_WIDTH, 3])\n",
    "\n",
    "    adv_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    # get processed enhanced image\n",
    "\n",
    "    enhanced = models.resnet(phone_image)\n",
    "\n",
    "    # transform both dslr and enhanced images to grayscale\n",
    "\n",
    "    enhanced_gray = tf.reshape(tf.image.rgb_to_grayscale(enhanced), [-1, PATCH_WIDTH * PATCH_HEIGHT])\n",
    "    dslr_gray = tf.reshape(tf.image.rgb_to_grayscale(dslr_image),[-1, PATCH_WIDTH * PATCH_HEIGHT])\n",
    "\n",
    "    # push randomly the enhanced or dslr image to an adversarial CNN-discriminator\n",
    "\n",
    "    adversarial_ = tf.multiply(enhanced_gray, 1 - adv_) + tf.multiply(dslr_gray, adv_)\n",
    "    adversarial_image = tf.reshape(adversarial_, [-1, PATCH_HEIGHT, PATCH_WIDTH, 1])\n",
    "\n",
    "    discrim_predictions = models.adversarial(adversarial_image)\n",
    "\n",
    "    # losses\n",
    "    # 1) texture (adversarial) loss\n",
    "\n",
    "    discrim_target = tf.concat([adv_, 1 - adv_], 1)\n",
    "\n",
    "    loss_discrim = -tf.reduce_sum(discrim_target * tf.log(tf.clip_by_value(discrim_predictions, 1e-10, 1.0)))\n",
    "    loss_texture = -loss_discrim\n",
    "\n",
    "    correct_predictions = tf.equal(tf.argmax(discrim_predictions, 1), tf.argmax(discrim_target, 1))\n",
    "    discim_accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "\n",
    "    # 2) content loss\n",
    "\n",
    "    CONTENT_LAYER = 'relu5_4'\n",
    "\n",
    "    enhanced_vgg = vgg.net(vgg_dir, vgg.preprocess(enhanced * 255))\n",
    "    dslr_vgg = vgg.net(vgg_dir, vgg.preprocess(dslr_image * 255))\n",
    "\n",
    "    content_size = utils._tensor_size(dslr_vgg[CONTENT_LAYER]) * batch_size\n",
    "    loss_content = 2 * tf.nn.l2_loss(enhanced_vgg[CONTENT_LAYER] - dslr_vgg[CONTENT_LAYER]) / content_size\n",
    "\n",
    "    # 3) color loss\n",
    "\n",
    "    enhanced_blur = utils.blur(enhanced)\n",
    "    dslr_blur = utils.blur(dslr_image)\n",
    "\n",
    "    loss_color = tf.reduce_sum(tf.pow(dslr_blur - enhanced_blur, 2))/(2 * batch_size)\n",
    "\n",
    "    # 4) total variation loss\n",
    "\n",
    "    batch_shape = (batch_size, PATCH_WIDTH, PATCH_HEIGHT, 3)\n",
    "    tv_y_size = utils._tensor_size(enhanced[:,1:,:,:])\n",
    "    tv_x_size = utils._tensor_size(enhanced[:,:,1:,:])\n",
    "    y_tv = tf.nn.l2_loss(enhanced[:,1:,:,:] - enhanced[:,:batch_shape[1]-1,:,:])\n",
    "    x_tv = tf.nn.l2_loss(enhanced[:,:,1:,:] - enhanced[:,:,:batch_shape[2]-1,:])\n",
    "    loss_tv = 2 * (x_tv/tv_x_size + y_tv/tv_y_size) / batch_size\n",
    "    \n",
    "    # 5) nima loss\n",
    "    base_model = VGG16(input_shape=(None, None, 3), include_top=False, pooling='avg', weights=None)\n",
    "    x = Dropout(0.75)(base_model.output)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    nima_model = Model(base_model.input, x)\n",
    "    nima_model.trainable = False\n",
    "    for layer in nima_model.layers:\n",
    "        layer.trainable = False\n",
    "    nima_outputs = nima_model(nima_preprocess_input(enhanced * 255.))\n",
    "    nima_score = tf.reduce_mean(mean_score(nima_outputs))\n",
    "    loss_nima = 9.0 - nima_score\n",
    "\n",
    "    # final loss\n",
    "\n",
    "    loss_generator = w_content * loss_content + w_texture * loss_texture + w_color * loss_color + w_tv * loss_tv + w_nima*loss_nima\n",
    "\n",
    "    # psnr loss\n",
    "\n",
    "    enhanced_flat = tf.reshape(enhanced, [-1, PATCH_SIZE])\n",
    "\n",
    "    loss_mse = tf.reduce_sum(tf.pow(dslr_ - enhanced_flat, 2))/(PATCH_SIZE * batch_size)\n",
    "    loss_psnr = 20 * utils.log10(1.0 / tf.sqrt(loss_mse))\n",
    "\n",
    "    # optimize parameters of image enhancement (generator) and discriminator networks\n",
    "\n",
    "    generator_vars = [v for v in tf.global_variables() if v.name.startswith(\"generator\")]\n",
    "    discriminator_vars = [v for v in tf.global_variables() if v.name.startswith(\"discriminator\")]\n",
    "\n",
    "    train_step_gen = tf.train.AdamOptimizer(learning_rate).minimize(loss_generator, var_list=generator_vars)\n",
    "    train_step_disc = tf.train.AdamOptimizer(learning_rate).minimize(loss_discrim, var_list=discriminator_vars)\n",
    "\n",
    "    saver = tf.train.Saver(var_list=generator_vars, max_to_keep=100)\n",
    "\n",
    "    print('Initializing variables')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Loading weights of nima')\n",
    "    nima_model.load_weights('../myproject/NIMA_weights/vgg16_weights.h5')\n",
    "\n",
    "    print('Training network')\n",
    "\n",
    "    train_loss_gen = 0.0\n",
    "    train_acc_discrim = 0.0\n",
    "\n",
    "    all_zeros = np.reshape(np.zeros((batch_size, 1)), [batch_size, 1])\n",
    "    test_idx = np.random.randint(0, TEST_SIZE, 5)\n",
    "    test_crops = test_data[test_idx, :]\n",
    "    test_dslr_crops = test_answ[test_idx, :]\n",
    "\n",
    "    logs = open(models_dir + phone + postfix + '.txt', \"w+\")\n",
    "    logs.close()\n",
    "\n",
    "    for i in range(num_train_iters):\n",
    "\n",
    "        # train generator\n",
    "\n",
    "        idx_train = np.random.randint(0, train_size, batch_size)\n",
    "\n",
    "        phone_images = train_data[idx_train]\n",
    "        dslr_images = train_answ[idx_train]\n",
    "\n",
    "        [loss_temp, temp] = sess.run([loss_generator, train_step_gen],\n",
    "                                        feed_dict={phone_: phone_images, dslr_: dslr_images, adv_: all_zeros, K.learning_phase():0})\n",
    "        train_loss_gen += loss_temp / eval_step\n",
    "\n",
    "        # train discriminator\n",
    "\n",
    "        idx_train = np.random.randint(0, train_size, batch_size)\n",
    "\n",
    "        # generate image swaps (dslr or enhanced) for discriminator\n",
    "        swaps = np.reshape(np.random.randint(0, 2, batch_size), [batch_size, 1])\n",
    "\n",
    "        phone_images = train_data[idx_train]\n",
    "        dslr_images = train_answ[idx_train]\n",
    "\n",
    "        [accuracy_temp, temp] = sess.run([discim_accuracy, train_step_disc],\n",
    "                                        feed_dict={phone_: phone_images, dslr_: dslr_images, adv_: swaps, K.learning_phase():0})\n",
    "        train_acc_discrim += accuracy_temp / eval_step\n",
    "\n",
    "        if i % eval_step == 0 or i == num_train_iters-1:\n",
    "\n",
    "            # test generator and discriminator CNNs\n",
    "\n",
    "            test_losses_gen = np.zeros((1, 7))\n",
    "            test_accuracy_disc = 0.0\n",
    "            loss_ssim = 0.0\n",
    "            nima_mean_score = 0.0\n",
    "\n",
    "            for j in range(num_test_batches):\n",
    "\n",
    "                be = j * batch_size\n",
    "                en = (j+1) * batch_size\n",
    "\n",
    "                swaps = np.reshape(np.random.randint(0, 2, batch_size), [batch_size, 1])\n",
    "\n",
    "                phone_images = test_data[be:en]\n",
    "                dslr_images = test_answ[be:en]\n",
    "\n",
    "                [enhanced_crops, accuracy_disc, losses, nima_mean] = sess.run([enhanced, discim_accuracy, \\\n",
    "                                [loss_generator, loss_content, loss_color, loss_texture, loss_tv, loss_psnr, loss_nima], nima_score], \\\n",
    "                                feed_dict={phone_: phone_images, dslr_: dslr_images, adv_: swaps, K.learning_phase():0})\n",
    "\n",
    "                test_losses_gen += np.asarray(losses) / num_test_batches\n",
    "                test_accuracy_disc += accuracy_disc / num_test_batches\n",
    "                nima_mean_score += nima_mean / num_test_batches\n",
    "\n",
    "                loss_ssim += MultiScaleSSIM(np.reshape(dslr_images * 255, [batch_size, PATCH_HEIGHT, PATCH_WIDTH, 3]),\n",
    "                                                    enhanced_crops * 255) / num_test_batches\n",
    "\n",
    "            logs_disc = \"step %d, %s | discriminator accuracy | train: %.4g, test: %.4g\" % \\\n",
    "                  (i, phone, train_acc_discrim, test_accuracy_disc)\n",
    "\n",
    "            logs_gen = \"generator losses | train: %.4g, test: %.4g | content: %.4g, color: %.4g, texture: %.4g, tv: %.4g | \\\n",
    "                        psnr: %.4g, nima: %.4g ssim: %.4g\\n, nima_mean:%.4g\" % \\\n",
    "                  (train_loss_gen, test_losses_gen[0][0], test_losses_gen[0][1], test_losses_gen[0][2],\n",
    "                   test_losses_gen[0][3], test_losses_gen[0][4], test_losses_gen[0][5], test_losses_gen[0][6],loss_ssim, nima_mean_score)\n",
    "\n",
    "            print(logs_disc)\n",
    "            print(logs_gen)\n",
    "\n",
    "            # save the results to log file\n",
    "\n",
    "            logs = open(models_dir + phone + postfix + '.txt', \"a\")\n",
    "            logs.write(logs_disc)\n",
    "            logs.write('\\n')\n",
    "            logs.write(logs_gen)\n",
    "            logs.write('\\n')\n",
    "            logs.close()\n",
    "\n",
    "            # save visual results for several test image crops\n",
    "\n",
    "            enhanced_crops = sess.run(enhanced, feed_dict={phone_: test_crops, dslr_: dslr_images, adv_: all_zeros, K.learning_phase():0})\n",
    "\n",
    "            idx = 0\n",
    "            for crop in enhanced_crops:\n",
    "                before_after = np.hstack((np.reshape(test_crops[idx], [PATCH_HEIGHT, PATCH_WIDTH, 3]), crop,\n",
    "                                          np.reshape(test_dslr_crops[idx], [PATCH_HEIGHT, PATCH_WIDTH, 3]))\n",
    "                misc.imsave(results_dir + str(phone)+ postfix+ \"_\" + str(idx) + '_iteration_' + str(i) + '.jpg', before_after)\n",
    "                idx += 1\n",
    "\n",
    "            train_loss_gen = 0.0\n",
    "            train_acc_discrim = 0.0\n",
    "\n",
    "            # save the model that corresponds to the current iteration\n",
    "\n",
    "            saver.save(sess, models_dir + str(phone)+'_withoutNIMA' + '_iteration_' + str(i) + '.ckpt', write_meta_graph=False)\n",
    "\n",
    "            # reload a different batch of training data\n",
    "\n",
    "            del train_data\n",
    "            del train_answ\n",
    "            train_data, train_answ = load_batch(phone, dped_dir, train_size, PATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
